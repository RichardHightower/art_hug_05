{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 5: Tokenization - The Gateway to Transformer Understanding\n\nThis notebook contains all the tokenization examples from article5.md, plus additional examples from Chapter 3 that were mentioned to be explained in Chapter 5.\n\n## Table of Contents\n1. [Environment Setup](#setup)\n2. [Basic Tokenization](#basic)\n3. [Tokenization Algorithms (BPE, WordPiece, Unigram)](#algorithms)\n4. [Custom Tokenization](#custom)\n5. [Debugging and Visualization](#debugging)\n6. [Multimodal Tokenization](#multimodal)\n7. [Chapter 3 Advanced Examples](#chapter3)\n8. [Exercises](#exercises)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a id='setup'></a>\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoImageProcessor,\n",
    "    CLIPProcessor,\n",
    "    AutoProcessor\n",
    ")\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Tokenization <a id='basic'></a>\n",
    "\n",
    "Let's start with the fundamental concepts of tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Tokenization Example\n",
    "\n",
    "Tokenization converts raw text into tokens and numerical IDs that models can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained fast tokenizer (BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Transformers are revolutionizing AI!\"\n",
    "\n",
    "# Tokenize and prepare model inputs in one step\n",
    "encoded = tokenizer(text)\n",
    "print('Input IDs:', encoded['input_ids'])\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n",
    "\n",
    "# For direct tensor output (e.g., for PyTorch models):\n",
    "tensor_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print('\\nTensor Input IDs:', tensor_inputs['input_ids'])\n",
    "print('Tensor shape:', tensor_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multilingual Tokenization with Emojis\n",
    "\n",
    "Modern tokenizers need to handle multiple languages and special characters like emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "text = \"Transformers est√°n revolucionando la IA! üöÄ\"\n",
    "\n",
    "# Tokenize and map to IDs in one step (recommended)\n",
    "encoded = tokenizer(text, return_tensors='pt')\n",
    "print('Input IDs:', encoded['input_ids'])\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(encoded['input_ids'][0]))\n",
    "\n",
    "# Inspect special tokens\n",
    "print('\\nSpecial tokens:', tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Batch Tokenization with Padding and Alignment\n",
    "\n",
    "For efficient processing, we often tokenize multiple texts at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentences = [\n",
    "    \"Tokenization is fun!\",\n",
    "    \"Let's build smarter models.\"\n",
    "]\n",
    "\n",
    "# Tokenize the batch, including alignment info\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,                # Pad to the longest sentence\n",
    "    truncation=True,             # Truncate if too long\n",
    "    return_tensors='pt',         # PyTorch tensors\n",
    "    return_offsets_mapping=True  # Get character-to-token alignment\n",
    ")\n",
    "\n",
    "print('Input IDs shape:', encoded['input_ids'].shape)\n",
    "print('\\nInput IDs:')\n",
    "print(encoded['input_ids'])\n",
    "print('\\nAttention Mask:')\n",
    "print(encoded['attention_mask'])\n",
    "\n",
    "# Show tokens for each sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])\n",
    "    print(f'\\nSentence {i+1} tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Special Token Handling\n",
    "\n",
    "Special tokens like [CLS], [SEP], and custom tokens are crucial for many transformer tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Inspect current special tokens\n",
    "print('Special tokens:', tokenizer.special_tokens_map)\n",
    "print('\\nSpecial token IDs:')\n",
    "for token_name, token in tokenizer.special_tokens_map.items():\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {token_name}: '{token}' -> ID: {token_id}\")\n",
    "\n",
    "# Add custom special tokens if needed\n",
    "special_tokens_dict = {'additional_special_tokens': ['<CUSTOM>', '<MEDICAL>']}\n",
    "num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f'\\nAdded {num_added} special tokens.')\n",
    "\n",
    "# Visualize tokenization with special tokens\n",
    "text = \"Classify this sentence.\"\n",
    "encoded = tokenizer(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "print(f'\\nTokens with Special Tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization Algorithms <a id='algorithms'></a>\n",
    "\n",
    "Let's explore the three main tokenization algorithms: BPE, WordPiece, and Unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Byte Pair Encoding (BPE) - Used by GPT, RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa's BPE tokenizer\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = bpe_tokenizer.tokenize(text)\n",
    "print('BPE Tokens:', tokens)\n",
    "\n",
    "# Show how BPE handles various words\n",
    "test_words = ['tokenization', 'transformer', 'preprocessing', 'pneumothorax']\n",
    "print('\\nBPE tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = bpe_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 WordPiece - Used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT's WordPiece tokenizer\n",
    "wordpiece_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = wordpiece_tokenizer.tokenize(text)\n",
    "print('WordPiece Tokens:', tokens)\n",
    "print('Notice the ## prefix for subword continuations!')\n",
    "\n",
    "# Show how WordPiece handles various words\n",
    "print('\\nWordPiece tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = wordpiece_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Unigram - Used by XLNet, ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XLM-RoBERTa's Unigram tokenizer\n",
    "unigram_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = unigram_tokenizer.tokenize(text)\n",
    "print('Unigram Tokens:', tokens)\n",
    "\n",
    "# Show how Unigram handles various words\n",
    "print('\\nUnigram tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = unigram_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Algorithm Comparison\n",
    "\n",
    "Let's compare all three algorithms side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare algorithms on various text types\n",
    "test_texts = [\n",
    "    \"unhappiness\",\n",
    "    \"I love pizza! üçïüî•\",\n",
    "    \"COVID-19 pandemic\",\n",
    "    \"user@example.com\",\n",
    "    \"myocardial infarction\"\n",
    "]\n",
    "\n",
    "tokenizers = {\n",
    "    'BPE (RoBERTa)': AutoTokenizer.from_pretrained('roberta-base'),\n",
    "    'WordPiece (BERT)': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'Unigram (XLM-R)': AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "}\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nTokenizing: '{text}'\")\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"  {name}: {tokens} (length: {len(tokens)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Tokenization <a id='custom'></a>\n",
    "\n",
    "For specialized domains, you might need to train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training a Custom Tokenizer (Simple Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific medical texts\n",
    "medical_texts = [\n",
    "    \"Patient exhibits signs of pneumothorax.\",\n",
    "    \"CT scan reveals bilateral infiltrates.\",\n",
    "    \"Myocardial infarction confirmed via ECG.\",\n",
    "    \"Administered 5mg of morphine for pain management.\",\n",
    "    \"Post-operative recovery progressing normally.\",\n",
    "    \"CBC shows elevated white blood cell count.\",\n",
    "    \"MRI indicates herniated disc at L4-L5.\",\n",
    "    \"Patient history includes hypertension and diabetes.\",\n",
    "    \"Prescribed antibiotics for bacterial infection.\",\n",
    "    \"Radiology report shows no acute findings.\",\n",
    "    \"Chronic obstructive pulmonary disease exacerbation.\",\n",
    "    \"Electrocardiogram shows atrial fibrillation.\"\n",
    "]\n",
    "\n",
    "# Start with a base tokenizer as template\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Train a new tokenizer on domain data\n",
    "custom_tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    medical_texts,\n",
    "    vocab_size=1000,\n",
    ")\n",
    "\n",
    "# Test the custom tokenizer\n",
    "test_text = \"Patient exhibits signs of pneumothorax.\"\n",
    "print(\"Original BERT tokenization:\")\n",
    "print(base_tokenizer.tokenize(test_text))\n",
    "print(\"\\nCustom medical tokenization:\")\n",
    "print(custom_tokenizer.tokenize(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training a Custom BPE Tokenizer (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Initialize a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Pre-tokenization (splitting on whitespace and punctuation)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=1000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Train from our medical corpus\n",
    "tokenizer.train_from_iterator(medical_texts, trainer=trainer)\n",
    "\n",
    "# Add post-processing for BERT-style tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 2),\n",
    "        (\"[SEP]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Test the custom BPE tokenizer\n",
    "test_text = \"Patient with myocardial infarction\"\n",
    "encoding = tokenizer.encode(test_text)\n",
    "print(f\"BPE tokens: {encoding.tokens}\")\n",
    "print(f\"BPE IDs: {encoding.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing General vs Domain-Specific Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical terms that might be split differently\n",
    "medical_terms = [\n",
    "    \"pneumothorax\",\n",
    "    \"myocardial\",\n",
    "    \"electrocardiogram\",\n",
    "    \"thrombocytopenia\",\n",
    "    \"cholecystectomy\"\n",
    "]\n",
    "\n",
    "# Load general tokenizer\n",
    "general_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Comparing tokenization of medical terms:\")\n",
    "print(\"=\" * 50)\n",
    "for term in medical_terms:\n",
    "    general_tokens = general_tokenizer.tokenize(term)\n",
    "    custom_tokens = custom_tokenizer.tokenize(term)\n",
    "    \n",
    "    print(f\"\\n'{term}':\")\n",
    "    print(f\"  General BERT: {general_tokens} (length: {len(general_tokens)})\")\n",
    "    print(f\"  Custom Medical: {custom_tokens} (length: {len(custom_tokens)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Debugging and Visualization <a id='debugging'></a>\n",
    "\n",
    "Understanding how tokenization works is crucial for debugging NLP pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing Tokenization with Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Let's test: ü§ñ transformers!\"\n",
    "output = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=None\n",
    ")\n",
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "offsets = output['offset_mapping']\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(\"\\nToken breakdown:\")\n",
    "print(\"-\" * 50)\n",
    "for token, (start, end) in zip(tokens, offsets):\n",
    "    if start == end:  # Special tokens\n",
    "        print(f\"  {token:15} [SPECIAL TOKEN]\")\n",
    "    else:\n",
    "        print(f\"  {token:15} [{start:2}, {end:2}] -> '{text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detecting Tokenizer-Model Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using a mismatched tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_model = AutoModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"Tokenization mismatch!\"\n",
    "inputs = bert_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Using BERT tokenizer with RoBERTa model:\")\n",
    "print(f\"BERT tokens: {bert_tokenizer.tokenize(text)}\")\n",
    "print(f\"BERT special tokens: {bert_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Show the correct pairing\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(f\"\\nRoBERTa tokens: {roberta_tokenizer.tokenize(text)}\")\n",
    "print(f\"RoBERTa special tokens: {roberta_tokenizer.special_tokens_map}\")\n",
    "print(\"\\n‚ö†Ô∏è  Notice the different special tokens and tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyzing Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text with potentially unknown tokens\n",
    "test_texts = [\n",
    "    \"Normal English text\",\n",
    "    \"Emojis: üòÄ üöÄ ü§ñ\",\n",
    "    \"Special chars: ‚Ñ¢ ¬© ¬Æ ¬µ\",\n",
    "    \"Mixed: Hello‰∏ñÁïåBonjour\",\n",
    "    \"Medical: pneumonoultramicroscopicsilicovolcanoconiosis\",\n",
    "    \"Code: def foo(x): return x**2\",\n",
    "    \"Email: user@example.com\",\n",
    "    \"URL: https://example.com/path\"\n",
    "]\n",
    "\n",
    "print(\"Analyzing unknown token generation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Check for unknown tokens\n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    unk_count = token_ids.count(unk_token_id)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    if unk_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Contains {unk_count} unknown tokens!\")\n",
    "        unk_positions = [i for i, tid in enumerate(token_ids) if tid == unk_token_id]\n",
    "        print(f\"  Unknown at positions: {unk_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Debugging Padding and Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sentences of different lengths\n",
    "sentences = [\n",
    "    \"Short\",\n",
    "    \"This is a medium length sentence.\",\n",
    "    \"This is a much longer sentence that will definitely exceed the maximum length limit we set for truncation testing purposes.\"\n",
    "]\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "print(f\"Testing padding and truncation (max_length={max_length})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different padding strategies\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])\n",
    "    print(f\"\\nOriginal: '{sent[:30]}...'\" if len(sent) > 30 else f\"\\nOriginal: '{sent}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Attention mask: {encoded['attention_mask'][i].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multimodal Tokenization <a id='multimodal'></a>\n",
    "\n",
    "Modern transformers can process not just text, but also images and other modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Image Tokenization with Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image for testing\n",
    "def create_sample_image():\n",
    "    image = Image.new('RGB', (224, 224), color='red')\n",
    "    # Add some variation\n",
    "    pixels = image.load()\n",
    "    for i in range(0, 224, 20):\n",
    "        for j in range(0, 224, 20):\n",
    "            pixels[i, j] = (0, 255, 0)  # Green dots\n",
    "    return image\n",
    "\n",
    "# Load a vision processor (tokenizer for images)\n",
    "processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Create sample image\n",
    "image = create_sample_image()\n",
    "print(f\"Image size: {image.size}\")\n",
    "\n",
    "# Process image into model-ready inputs\n",
    "inputs = processor(images=image, return_tensors='pt')\n",
    "print(f'\\nPixel values shape: {inputs[\"pixel_values\"].shape}')\n",
    "\n",
    "# Vision Transformer details\n",
    "patch_size = 16\n",
    "image_size = 224\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "print(f\"\\nViT tokenization details:\")\n",
    "print(f\"  Number of image patches: {num_patches}\")\n",
    "print(f\"  Each patch: {patch_size}x{patch_size} pixels\")\n",
    "print(f\"  Sequence length: {num_patches + 1} (patches + [CLS] token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 CLIP Multimodal Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP processor (handles both text and images)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Create sample image\n",
    "image = create_sample_image()\n",
    "\n",
    "# Sample texts for similarity comparison\n",
    "texts = [\n",
    "    \"a red square with green dots\",\n",
    "    \"a photo of a cat\", \n",
    "    \"a photo of a dog\",\n",
    "    \"a colorful pattern\"\n",
    "]\n",
    "\n",
    "# Process both text and images together\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"CLIP multimodal inputs:\")\n",
    "print(f\"  Text input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"  Image input shape: {inputs['pixel_values'].shape}\")\n",
    "\n",
    "# Show text tokenization\n",
    "print(\"\\nText tokenization:\")\n",
    "for i, text in enumerate(texts):\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(inputs['input_ids'][i])\n",
    "    # Filter out padding tokens for display\n",
    "    tokens = [t for t in tokens if t != processor.tokenizer.pad_token]\n",
    "    print(f\"  '{text}': {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Comparing Different Image Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different image processors/tokenizers\n",
    "image = create_sample_image()\n",
    "\n",
    "processors = {\n",
    "    \"ViT\": \"google/vit-base-patch16-224\",\n",
    "    \"CLIP\": \"openai/clip-vit-base-patch32\",\n",
    "    \"DeiT\": \"facebook/deit-base-patch16-224\"\n",
    "}\n",
    "\n",
    "print(\"Comparing image processors:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model_name in processors.items():\n",
    "    try:\n",
    "        if name == \"CLIP\":\n",
    "            processor = CLIPProcessor.from_pretrained(model_name)\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        else:\n",
    "            processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        print(f\"\\n{name} processor ({model_name}):\")\n",
    "        print(f\"  Input shape: {inputs['pixel_values'].shape}\")\n",
    "        \n",
    "        # Get processor configuration\n",
    "        if hasattr(processor, 'size'):\n",
    "            print(f\"  Expected size: {processor.size}\")\n",
    "        if hasattr(processor, 'do_normalize'):\n",
    "            print(f\"  Normalization: {processor.do_normalize}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "## 8. Exercises <a id='exercises'></a>\n\nNow let's implement the exercises from article5.md",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.8 Performance Comparison of Tokenizers\n\nLet's compare the performance of different tokenizers to understand speed vs. functionality tradeoffs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Out-of-Vocabulary (OOV) Handling\nprint(\"=== Out-of-Vocabulary (OOV) Word Handling ===\")\n\n# Test with made-up and rare words\ntest_texts = [\n    \"The flibbertigibbet jumped over the moon.\",\n    \"Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease.\",\n    \"The ü¶Ñ and üåà are beautiful.\",\n    \"Contact us at support@‰ºÅ‰∏ö.com\",\n]\n\ntokenizers_to_test = {\n    \"BERT\": bert_tokenizer,\n    \"GPT-2\": gpt2_tokenizer,\n    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n}\n\nfor text in test_texts:\n    print(f\"\\nText: '{text}'\")\n    print(\"-\" * 70)\n    \n    for name, tokenizer in tokenizers_to_test.items():\n        # Get UNK token for this tokenizer\n        unk_token = getattr(tokenizer, 'unk_token', None)\n        \n        # Tokenize\n        if name == \"BERT\":\n            tokens = tokenizer.tokenize(text.lower())\n        else:\n            tokens = tokenizer.tokenize(text)\n        \n        # Check for UNK tokens\n        unk_count = tokens.count(unk_token) if unk_token else 0\n        \n        print(f\"{name:10} ({len(tokens):2} tokens): \", end=\"\")\n        if unk_count > 0:\n            print(f\"‚ö†Ô∏è  {unk_count} UNK token(s)! \", end=\"\")\n        \n        # Show first few tokens\n        display_tokens = tokens[:8] + [\"...\"] if len(tokens) > 8 else tokens\n        print(display_tokens)\n\n# Demonstrate subword handling of OOV\nprint(\"\\n=== Subword Decomposition of OOV Words ===\")\nmade_up_word = \"supersupercalifragilisticexpialidocious\"\n\nprint(f\"\\nMade-up word: '{made_up_word}'\")\nprint(\"\\nHow different tokenizers handle it:\")\nprint(f\"BERT:     {bert_tokenizer.tokenize(made_up_word)}\")\nprint(f\"GPT-2:    {gpt2_tokenizer.tokenize(made_up_word)}\")\nprint(f\"RoBERTa:  {tokenizers_to_test['RoBERTa'].tokenize(made_up_word)}\")\n\n# Key insight\nprint(\"\\nüí° Key Insight:\")\nprint(\"- BERT uses [UNK] tokens for unknown words/characters\")\nprint(\"- GPT-2 and RoBERTa use BPE to break down any word into known subwords\")\nprint(\"- This is why BPE-based models handle OOV words better!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.7 Out-of-Vocabulary (OOV) Handling Strategies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install tiktoken if not already installed\ntry:\n    import tiktoken\nexcept ImportError:\n    print(\"Installing tiktoken...\")\n    import subprocess\n    subprocess.check_call([\"pip\", \"install\", \"tiktoken\"])\n    import tiktoken\n\n# Comparing with TikToken (used by GPT-3.5/4)\nprint(\"=== Comparing HuggingFace Tokenizers with TikToken ===\")\n\n# Initialize tiktoken\nencoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 encoding\n\n# Test texts\ntest_texts = [\n    \"Hello world!\",\n    \"The transformer architecture revolutionized NLP in 2017.\",\n    \"def tokenize(text): return text.split()\",\n    \"Email: user@example.com, URL: https://example.com\",\n]\n\n# Compare tokenization\nfor text in test_texts:\n    print(f\"\\nText: '{text}'\")\n    print(\"-\" * 60)\n    \n    # TikToken\n    tiktoken_ids = encoding.encode(text)\n    tiktoken_tokens = [encoding.decode([tid]) for tid in tiktoken_ids]\n    print(f\"TikToken (GPT-3.5/4): {tiktoken_tokens} ({len(tiktoken_tokens)} tokens)\")\n    \n    # GPT-2\n    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n    print(f\"GPT-2 (BPE): {gpt2_tokens} ({len(gpt2_tokens)} tokens)\")\n    \n    # BERT\n    bert_tokens = bert_tokenizer.tokenize(text.lower())\n    print(f\"BERT (WordPiece): {bert_tokens} ({len(bert_tokens)} tokens)\")\n\n# Vocabulary size comparison\nprint(\"\\n=== Vocabulary Size Comparison ===\")\nprint(f\"TikToken (cl100k_base): {encoding.n_vocab:,} tokens\")\nprint(f\"GPT-2: {gpt2_tokenizer.vocab_size:,} tokens\")\nprint(f\"BERT: {bert_tokenizer.vocab_size:,} tokens\")\nprint(f\"T5: {t5_tokenizer.vocab_size:,} tokens\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.6 Comparing Tokenizers with TikToken (GPT-3.5/4)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Subword Tokenization Deep Dive\nprint(\"=== Subword Tokenization Methods Comparison ===\")\n\n# Example text with various challenges\ntext = (\n    \"Tokenization is fundamental to NLP. Let's explore BPE, WordPiece, and \"\n    \"SentencePiece algorithms!\"\n)\n\n# 1. BPE (Byte Pair Encoding) - GPT-2\nprint(\"\\n1. BPE Tokenization (GPT-2):\")\ngpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ngpt2_tokens = gpt2_tokenizer.tokenize(text)\ngpt2_ids = gpt2_tokenizer.encode(text)\nprint(f\"   Tokens: {gpt2_tokens}\")\nprint(f\"   Token count: {len(gpt2_tokens)}\")\nprint(f\"   Vocabulary size: {gpt2_tokenizer.vocab_size}\")\n\n# 2. WordPiece - BERT\nprint(\"\\n2. WordPiece Tokenization (BERT):\")\nbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_tokens = bert_tokenizer.tokenize(text)\nbert_ids = bert_tokenizer.encode(text)\nprint(f\"   Tokens: {bert_tokens}\")\nprint(f\"   Token count: {len(bert_tokens)}\")\nprint(f\"   Notice '##' prefix for subword continuations\")\n\n# 3. SentencePiece - T5\nprint(\"\\n3. SentencePiece Tokenization (T5):\")\nt5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nt5_tokens = t5_tokenizer.tokenize(text)\nt5_ids = t5_tokenizer.encode(text)\nprint(f\"   Tokens: {t5_tokens}\")\nprint(f\"   Token count: {len(t5_tokens)}\")\nprint(f\"   Notice '‚ñÅ' for word boundaries\")\n\n# Handling unknown/rare words\nprint(\"\\n=== Handling Unknown/Rare Words ===\")\nrare_word = \"Supercalifragilisticexpialidocious\"\n\nprint(f\"\\nRare word: '{rare_word}'\")\nprint(f\"GPT-2 (BPE): {gpt2_tokenizer.tokenize(rare_word)}\")\nprint(f\"BERT (WordPiece): {bert_tokenizer.tokenize(rare_word.lower())}\")\nprint(f\"T5 (SentencePiece): {t5_tokenizer.tokenize(rare_word)}\")\n\n# Demonstrate vocabulary lookup\nprint(\"\\n=== Vocabulary Lookup Example ===\")\nword = \"tokenization\"\nprint(f\"Looking up '{word}':\")\n\n# Check if whole word is in vocabulary\nif word in gpt2_tokenizer.vocab:\n    print(f\"  GPT-2: '{word}' is in vocabulary with ID {gpt2_tokenizer.vocab[word]}\")\nelse:\n    print(f\"  GPT-2: '{word}' not in vocabulary, will be split into subwords\")\n\n# For BERT (lowercase)\nword_lower = word.lower()\nif word_lower in bert_tokenizer.vocab:\n    print(f\"  BERT: '{word_lower}' is in vocabulary with ID {bert_tokenizer.vocab[word_lower]}\")\nelse:\n    print(f\"  BERT: '{word_lower}' not in vocabulary, will be split into subwords\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.5 Subword Tokenization Deep Dive\n\nLet's explore how different subword tokenization methods handle complex words and why it matters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Token-to-Character Offset Mapping\nprint(\"=== Token-to-Character Offset Mapping ===\")\n\ntext = \"Hugging Face's tokenizers are extremely powerful!\"\nencoding = tokenizer(\n    text, \n    return_offsets_mapping=True, \n    add_special_tokens=True\n)\n\ntokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\noffsets = encoding['offset_mapping']\n\nprint(f\"Original text: '{text}'\")\nprint(f\"\\nToken to character mapping:\")\nprint(\"-\" * 60)\nprint(f\"{'Token':15} {'Text':20} {'Start':>6} {'End':>6}\")\nprint(\"-\" * 60)\n\nfor token, (start, end) in zip(tokens, offsets):\n    if start == end:  # Special tokens have (0, 0) offsets\n        print(f\"{token:15} {'[SPECIAL TOKEN]':20} {start:6} {end:6}\")\n    else:\n        original_text = text[start:end]\n        print(f\"{token:15} {original_text:20} {start:6} {end:6}\")\n\n# Practical use case: Highlighting entities\nprint(\"\\n=== Practical Example: Entity Highlighting ===\")\n\n# Simulate NER predictions (token indices for \"Hugging Face\")\nentity_token_indices = [1, 2]  # Tokens at positions 1 and 2\n\nprint(\"Detected entity tokens:\")\nentity_chars = []\nfor idx in entity_token_indices:\n    token = tokens[idx]\n    start, end = offsets[idx]\n    entity_chars.extend(range(start, end))\n    print(f\"  Token '{token}' -> '{text[start:end]}'\")\n\n# Reconstruct the entity from character positions\nmin_char = min(entity_chars)\nmax_char = max(entity_chars) + 1\nentity_text = text[min_char:max_char]\nprint(f\"\\nExtracted entity: '{entity_text}'\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.4 Token-to-Character Offset Mapping\n\nOffset mapping is crucial for tasks like Named Entity Recognition where you need to map model predictions back to the original text.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multiple Sequences for Question-Answering\nprint(\"=== Multiple Sequences (Question-Answering) ===\")\n\nquestion = \"What is tokenization?\"\ncontext = (\n    \"Tokenization is the process of breaking down text into smaller units \"\n    \"called tokens. These tokens can be words, subwords, or even characters. \"\n    \"In NLP, tokenization is a crucial preprocessing step that converts \"\n    \"raw text into a format that machine learning models can understand.\"\n)\n\n# Encode question and context together\nqa_encoding = tokenizer(\n    question, context, \n    padding=True, \n    truncation=True, \n    return_tensors=\"pt\"\n)\n\n# Convert to tokens to visualize\ntokens = tokenizer.convert_ids_to_tokens(qa_encoding[\"input_ids\"][0])\ntoken_type_ids = qa_encoding['token_type_ids'][0].tolist()\n\nprint(f\"Question: {question}\")\nprint(f\"Context: {context[:100]}...\")\nprint(f\"\\nCombined tokens (first 20): {tokens[:20]}...\")\nprint(f\"\\nToken type IDs visualization:\")\nprint(\"  0 = Question/First sequence\")\nprint(\"  1 = Context/Second sequence\")\n\n# Visualize token types\nfor i in range(min(20, len(tokens))):\n    print(f\"  Token {i:2d}: '{tokens[i]:15}' -> Type {token_type_ids[i]}\")\n\n# Find where question ends and context begins\nsep_positions = [i for i, token in enumerate(tokens) if token == '[SEP]']\nprint(f\"\\n[SEP] token positions: {sep_positions}\")\nprint(f\"Question ends at position: {sep_positions[0]}\")\nprint(f\"Context starts at position: {sep_positions[0] + 1}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.3 Handling Multiple Sequences (Question-Answering Example)\n\nMany NLP tasks require processing multiple sequences together, like question-answering or text entailment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced Truncation Examples\nprint(\"=== Advanced Truncation Examples ===\")\n\n# Create a very long text\nlong_text = \" \".join([\"This is a very long sentence.\"] * 50)\n\n# Without truncation (will be very long)\ntokens_no_trunc = tokenizer.tokenize(long_text)\nprint(f\"Without truncation: {len(tokens_no_trunc)} tokens\")\n\n# With truncation to max_length\ntokens_with_trunc = tokenizer(\n    long_text, truncation=True, max_length=20, return_tensors=\"pt\"\n)\nprint(f\"With truncation (max_length=20): {tokens_with_trunc['input_ids'].shape[1]} tokens\")\n\n# Show truncated tokens\ntruncated_tokens = tokenizer.convert_ids_to_tokens(tokens_with_trunc['input_ids'][0].tolist())\nprint(f\"Truncated tokens: {truncated_tokens}\")\n\n# Truncation strategies for sentence pairs\nprint(\"\\n=== Truncation Strategies for Sentence Pairs ===\")\nquestion = \"What is the capital of France?\"\ncontext = \" \".join([\"Paris is the capital and most populous city of France.\"] * 10)\n\n# Strategy: 'only_second' - truncate only the context\nencoding_only_second = tokenizer(\n    question, context,\n    truncation='only_second',\n    max_length=50,\n    return_tensors='pt'\n)\nprint(f\"'only_second' strategy: {encoding_only_second['input_ids'].shape}\")\n\n# Strategy: 'longest_first' - truncate the longest sequence first\nencoding_longest_first = tokenizer(\n    question, context,\n    truncation='longest_first',\n    max_length=50,\n    return_tensors='pt'\n)\nprint(f\"'longest_first' strategy: {encoding_longest_first['input_ids'].shape}\")\n\n# Show which parts were kept\ntokens_only_second = tokenizer.convert_ids_to_tokens(encoding_only_second['input_ids'][0])\nprint(f\"\\nTokens with 'only_second' (first 10): {tokens_only_second[:10]}...\")\nprint(f\"Question preserved: {'what' in ' '.join(tokens_only_second).lower()}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Advanced Truncation Strategies\n\nTruncation is essential when dealing with texts longer than the model's maximum sequence length.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced Padding Examples from Chapter 3\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# 1. Padding examples with different strategies\nprint(\"=== Advanced Padding Examples ===\")\ntexts = [\n    \"Short text.\",\n    \"This is a medium length sentence that demonstrates padding.\",\n    \"This is a much longer sentence that will show how padding works with \"\n    \"multiple sentences of different lengths in a batch.\",\n]\n\n# Padding to max length in batch\nbatch_encoding = tokenizer(texts, padding=True, return_tensors=\"pt\")\nprint(f\"Original texts lengths: {[len(text.split()) for text in texts]}\")\nprint(f\"Padded sequence lengths: {batch_encoding['input_ids'].shape}\")\nprint(f\"Attention mask shape: {batch_encoding['attention_mask'].shape}\")\nprint(f\"\\nAttention masks (1=real token, 0=padding):\")\nfor i, mask in enumerate(batch_encoding['attention_mask']):\n    print(f\"  Text {i+1}: {mask.tolist()}\")\n\n# Different padding strategies\nprint(\"\\n=== Padding Strategies ===\")\n\n# Dynamic padding (to longest in batch)\ndynamic_padding = tokenizer(texts, padding=\"longest\", return_tensors=\"pt\")\nprint(f\"Dynamic padding shape: {dynamic_padding['input_ids'].shape}\")\n\n# Fixed padding to specific length\nfixed_padding = tokenizer(texts, padding=\"max_length\", max_length=30, return_tensors=\"pt\")\nprint(f\"Fixed padding shape (max_length=30): {fixed_padding['input_ids'].shape}\")\n\n# No padding\nno_padding = tokenizer(texts, padding=False)\nprint(f\"No padding: {[len(ids) for ids in no_padding['input_ids']]}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.1 Advanced Padding and Truncation Strategies\n\nUnderstanding how padding and truncation work is crucial for batch processing.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Chapter 3 Advanced Tokenization Examples <a id='chapter3'></a>\n\nThese are the advanced tokenization examples from Chapter 3 that were deferred to Chapter 5 for deeper explanation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises <a id='exercises'></a>\n",
    "\n",
    "Now let's implement the exercises from article5.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenize Multilingual Sentences with Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Tokenize multilingual sentences including emojis and domain-specific terms\n",
    "\n",
    "multilingual_sentences = [\n",
    "    \"Hello world! üëã\",\n",
    "    \"Bonjour le monde! üá´üá∑\",\n",
    "    \"Hola mundo! üá™üá∏\",\n",
    "    \"‰Ω†Â•Ω‰∏ñÁïåÔºÅüá®üá≥\",\n",
    "    \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! üá∑üá∫\",\n",
    "    \"The patient has COVID-19 ü¶†\",\n",
    "    \"Machine learning is amazing! ü§ñüí°\"\n",
    "]\n",
    "\n",
    "# Use multilingual BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "print(\"Exercise 1: Multilingual Tokenization Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sentence in multilingual_sentences:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\nText: '{sentence}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    \n",
    "    # Check for unknown tokens\n",
    "    if tokenizer.unk_token in tokens:\n",
    "        print(\"‚ö†Ô∏è  Contains unknown tokens!\")\n",
    "\n",
    "# Analysis of unusual tokenization results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis:\")\n",
    "print(\"- Emojis are often tokenized as [UNK] or split into multiple tokens\")\n",
    "print(\"- Different scripts (Chinese, Cyrillic) are handled differently\")\n",
    "print(\"- Domain terms like 'COVID-19' may be split unexpectedly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train a Custom BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Train a custom BPE tokenizer on domain-specific corpus\n",
    "\n",
    "# Create a small domain-specific corpus (scientific abstracts)\n",
    "scientific_corpus = [\n",
    "    \"The quantum entanglement phenomenon demonstrates non-local correlations.\",\n",
    "    \"CRISPR-Cas9 enables precise genome editing in mammalian cells.\",\n",
    "    \"Machine learning algorithms optimize hyperparameters automatically.\",\n",
    "    \"Photosynthesis converts light energy into chemical energy.\",\n",
    "    \"Neurotransmitters facilitate synaptic transmission in neurons.\",\n",
    "    \"The Higgs boson was discovered at the Large Hadron Collider.\",\n",
    "    \"DNA polymerase synthesizes new DNA strands during replication.\",\n",
    "    \"Quantum computing leverages superposition and entanglement.\",\n",
    "    \"The mitochondria produces ATP through oxidative phosphorylation.\",\n",
    "    \"Climate change affects global temperature and precipitation patterns.\"\n",
    "]\n",
    "\n",
    "# Train custom tokenizer\n",
    "print(\"Training custom BPE tokenizer on scientific corpus...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "custom_tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    scientific_corpus,\n",
    "    vocab_size=1000\n",
    ")\n",
    "\n",
    "# Compare tokenization\n",
    "test_terms = [\n",
    "    \"quantum entanglement\",\n",
    "    \"CRISPR-Cas9\",\n",
    "    \"photosynthesis\",\n",
    "    \"neurotransmitters\"\n",
    "]\n",
    "\n",
    "print(\"\\nComparing standard GPT-2 vs custom scientific tokenizer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in test_terms:\n",
    "    standard_tokens = base_tokenizer.tokenize(term)\n",
    "    custom_tokens = custom_tokenizer.tokenize(term)\n",
    "    \n",
    "    print(f\"\\nTerm: '{term}'\")\n",
    "    print(f\"  Standard GPT-2: {standard_tokens} (length: {len(standard_tokens)})\")\n",
    "    print(f\"  Custom Scientific: {custom_tokens} (length: {len(custom_tokens)})\")\n",
    "\n",
    "print(\"\\nObservation: Custom tokenizer better preserves domain-specific terms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Identify and Fix Tokenization Mismatch Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Intentionally create and fix a tokenizer-model mismatch\n",
    "\n",
    "print(\"Exercise 3: Tokenizer-Model Mismatch Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create the mismatch\n",
    "print(\"\\nStep 1: Creating intentional mismatch...\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Note: We're not loading the full model to save memory in the notebook\n",
    "# In practice, you would use: roberta_model = AutoModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"Tokenization is crucial for NLP!\"\n",
    "\n",
    "# Show the problem\n",
    "bert_tokens = bert_tokenizer(text, return_tensors='pt')\n",
    "print(f\"BERT tokenizer output:\")\n",
    "print(f\"  Tokens: {bert_tokenizer.tokenize(text)}\")\n",
    "print(f\"  Special tokens: {list(bert_tokenizer.special_tokens_map.keys())}\")\n",
    "print(f\"  Input IDs shape: {bert_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Step 2: Fix the mismatch\n",
    "print(\"\\nStep 2: Fixing the mismatch...\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "roberta_tokens = roberta_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(f\"RoBERTa tokenizer output (correct):\")\n",
    "print(f\"  Tokens: {roberta_tokenizer.tokenize(text)}\")\n",
    "print(f\"  Special tokens: {list(roberta_tokenizer.special_tokens_map.keys())}\")\n",
    "print(f\"  Input IDs shape: {roberta_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Step 3: Key differences\n",
    "print(\"\\nStep 3: Key differences:\")\n",
    "print(\"- BERT uses [CLS] and [SEP] tokens\")\n",
    "print(\"- RoBERTa uses <s> and </s> tokens\")\n",
    "print(\"- Different vocabulary mappings\")\n",
    "print(\"- Different tokenization rules\")\n",
    "print(\"\\n‚úÖ Always match tokenizer with model architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Visualize Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Visualize special tokens for classification task\n",
    "\n",
    "print(\"Exercise 4: Special Tokens Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Text classification example\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Single sentence\n",
    "single_text = \"This movie is fantastic!\"\n",
    "single_encoded = tokenizer(single_text)\n",
    "\n",
    "print(\"Single sentence classification:\")\n",
    "print(f\"Text: '{single_text}'\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(single_encoded['input_ids'])\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"  - [CLS] at position 0: Classification token\")\n",
    "print(f\"  - Content tokens: {tokens[1:-1]}\")\n",
    "print(f\"  - [SEP] at position {len(tokens)-1}: Separator token\")\n",
    "\n",
    "# Sentence pair (for tasks like entailment)\n",
    "text_a = \"The weather is sunny.\"\n",
    "text_b = \"It's a beautiful day.\"\n",
    "pair_encoded = tokenizer(text_a, text_b)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"\\nSentence pair classification:\")\n",
    "print(f\"Text A: '{text_a}'\")\n",
    "print(f\"Text B: '{text_b}'\")\n",
    "pair_tokens = tokenizer.convert_ids_to_tokens(pair_encoded['input_ids'])\n",
    "token_type_ids = pair_encoded['token_type_ids']\n",
    "\n",
    "print(f\"\\nTokens with segment IDs:\")\n",
    "for i, (token, segment) in enumerate(zip(pair_tokens, token_type_ids)):\n",
    "    print(f\"  {i:2d}: '{token:15}' (segment {segment})\")\n",
    "\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"  - [CLS]: Start of sequence, used for classification\")\n",
    "print(f\"  - First [SEP]: Separates sentence A from sentence B\")\n",
    "print(f\"  - Second [SEP]: End of sequence\")\n",
    "print(f\"  - Segment 0: First sentence + special tokens\")\n",
    "print(f\"  - Segment 1: Second sentence + final [SEP]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Experiment with Noisy Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Tokenizing noisy text with different algorithms\n",
    "\n",
    "print(\"Exercise 5: Noisy Text Tokenization Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Noisy text examples\n",
    "noisy_texts = [\n",
    "    # Typos\n",
    "    \"I lvoe naturl langauge procesing\",\n",
    "    \"Thsi is vrey interessting\",\n",
    "    \n",
    "    # Slang and informal\n",
    "    \"gonna b late 2nite lol\",\n",
    "    \"ur awesome btw :)\",\n",
    "    \n",
    "    # Code snippets\n",
    "    \"def calculate_loss(y_true, y_pred): return mse(y_true, y_pred)\",\n",
    "    \"import torch.nn as nn\",\n",
    "    \n",
    "    # Mixed case and special chars\n",
    "    \"CamelCaseExample_with_underscores\",\n",
    "    \"email@domain.com | phone: +1-234-567-8900\"\n",
    "]\n",
    "\n",
    "# Load different tokenizers\n",
    "tokenizers = {\n",
    "    'BPE (GPT-2)': AutoTokenizer.from_pretrained('gpt2'),\n",
    "    'WordPiece (BERT)': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'Unigram (XLM-R)': AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "}\n",
    "\n",
    "for text in noisy_texts:\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text.lower() if 'uncased' in name else text)\n",
    "        results[name] = {\n",
    "            'tokens': tokens,\n",
    "            'length': len(tokens)\n",
    "        }\n",
    "        print(f\"{name}: {tokens} (length: {len(tokens)})\")\n",
    "    \n",
    "    # Find which handles it best (fewer tokens usually = better)\n",
    "    best = min(results.items(), key=lambda x: x[1]['length'])\n",
    "    print(f\"\\n‚úì Most efficient: {best[0]} with {best[1]['length']} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis:\")\n",
    "print(\"- BPE (GPT-2) handles contractions and slang well\")\n",
    "print(\"- WordPiece (BERT) struggles with typos and creates more [UNK] tokens\")\n",
    "print(\"- Unigram (XLM-R) provides a balance for multilingual text\")\n",
    "print(\"- Code snippets are challenging for all tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. **Basic Tokenization**: Converting text to tokens and IDs\n",
    "2. **Tokenization Algorithms**: BPE, WordPiece, and Unigram differences\n",
    "3. **Custom Tokenization**: Training tokenizers for specialized domains\n",
    "4. **Debugging Tools**: Visualizing and understanding tokenization\n",
    "5. **Multimodal Tokenization**: Processing images alongside text\n",
    "\n",
    "### Key Points to Remember:\n",
    "\n",
    "- Always match your tokenizer with your model\n",
    "- Custom tokenizers can significantly improve domain-specific performance\n",
    "- Different algorithms have different strengths (BPE for flexibility, WordPiece for consistency)\n",
    "- Special tokens are crucial for task-specific fine-tuning\n",
    "- Modern transformers can handle multiple modalities through specialized tokenization\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try training a custom tokenizer on your own domain data\n",
    "- Experiment with different tokenization strategies for your use case\n",
    "- Explore multimodal models like CLIP for vision-language tasks\n",
    "- Practice debugging tokenization issues in your NLP pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}