{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Tokenization - The Gateway to Transformer Understanding\n",
    "\n",
    "This notebook contains all the tokenization examples from article5.md, plus additional examples from Chapter 3 that were mentioned to be explained in Chapter 5.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#setup)\n",
    "2. [Basic Tokenization](#basic)\n",
    "3. [Tokenization Algorithms (BPE, WordPiece, Unigram)](#algorithms)\n",
    "4. [Custom Tokenization](#custom)\n",
    "5. [Debugging and Visualization](#debugging)\n",
    "6. [Multimodal Tokenization](#multimodal)\n",
    "7. [Chapter 3 Advanced Examples](#chapter3)\n",
    "8. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a id='setup'></a>\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoImageProcessor,\n",
    "    CLIPProcessor,\n",
    "    AutoProcessor\n",
    ")\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Tokenization <a id='basic'></a>\n",
    "\n",
    "Let's start with the fundamental concepts of tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Tokenization Example\n",
    "\n",
    "Tokenization converts raw text into tokens and numerical IDs that models can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 19081, 2024, 4329, 6026, 9932, 999, 102]\n",
      "Tokens: ['[CLS]', 'transformers', 'are', 'revolution', '##izing', 'ai', '!', '[SEP]']\n",
      "\n",
      "Tensor Input IDs: tensor([[  101, 19081,  2024,  4329,  6026,  9932,   999,   102]])\n",
      "Tensor shape: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained fast tokenizer (BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Transformers are revolutionizing AI!\"\n",
    "\n",
    "# Tokenize and prepare model inputs in one step\n",
    "encoded = tokenizer(text)\n",
    "print('Input IDs:', encoded['input_ids'])\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n",
    "\n",
    "# For direct tensor output (e.g., for PyTorch models):\n",
    "tensor_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print('\\nTensor Input IDs:', tensor_inputs['input_ids'])\n",
    "print('Tensor shape:', tensor_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multilingual Tokenization with Emojis\n",
    "\n",
    "Modern tokenizers need to handle multiple languages and special characters like emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 64562, 16037, 86095, 34522, 96458, 10605, 10109, 87769,   106,\n",
      "           100,   102]])\n",
      "Tokens: ['[CLS]', 'Transformers', 'est√°n', 'rev', '##olu', '##ciona', '##ndo', 'la', 'IA', '!', '[UNK]', '[SEP]']\n",
      "\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# Load multilingual tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "text = \"Transformers est√°n revolucionando la IA! üöÄ\"\n",
    "\n",
    "# Tokenize and map to IDs in one step (recommended)\n",
    "encoded = tokenizer(text, return_tensors='pt')\n",
    "print('Input IDs:', encoded['input_ids'])\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(encoded['input_ids'][0]))\n",
    "\n",
    "# Inspect special tokens\n",
    "print('\\nSpecial tokens:', tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Batch Tokenization with Padding and Alignment\n",
    "\n",
    "For efficient processing, we often tokenize multiple texts at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([2, 9])\n",
      "\n",
      "Input IDs:\n",
      "tensor([[  101, 19204,  3989,  2003,  4569,   999,   102,     0,     0],\n",
      "        [  101,  2292,  1005,  1055,  3857, 25670,  4275,  1012,   102]])\n",
      "\n",
      "Attention Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Sentence 1 tokens: ['[CLS]', 'token', '##ization', 'is', 'fun', '!', '[SEP]', '[PAD]', '[PAD]']\n",
      "\n",
      "Sentence 2 tokens: ['[CLS]', 'let', \"'\", 's', 'build', 'smarter', 'models', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentences = [\n",
    "    \"Tokenization is fun!\",\n",
    "    \"Let's build smarter models.\"\n",
    "]\n",
    "\n",
    "# Tokenize the batch, including alignment info\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,                # Pad to the longest sentence\n",
    "    truncation=True,             # Truncate if too long\n",
    "    return_tensors='pt',         # PyTorch tensors\n",
    "    return_offsets_mapping=True  # Get character-to-token alignment\n",
    ")\n",
    "\n",
    "print('Input IDs shape:', encoded['input_ids'].shape)\n",
    "print('\\nInput IDs:')\n",
    "print(encoded['input_ids'])\n",
    "print('\\nAttention Mask:')\n",
    "print(encoded['attention_mask'])\n",
    "\n",
    "# Show tokens for each sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])\n",
    "    print(f'\\nSentence {i+1} tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Special Token Handling\n",
    "\n",
    "Special tokens like [CLS], [SEP], and custom tokens are crucial for many transformer tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "\n",
      "Special token IDs:\n",
      "  unk_token: '[UNK]' -> ID: 100\n",
      "  sep_token: '[SEP]' -> ID: 102\n",
      "  pad_token: '[PAD]' -> ID: 0\n",
      "  cls_token: '[CLS]' -> ID: 101\n",
      "  mask_token: '[MASK]' -> ID: 103\n",
      "\n",
      "Added 2 special tokens.\n",
      "\n",
      "Tokens with Special Tokens: ['[CLS]', 'classify', 'this', 'sentence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Inspect current special tokens\n",
    "print('Special tokens:', tokenizer.special_tokens_map)\n",
    "print('\\nSpecial token IDs:')\n",
    "for token_name, token in tokenizer.special_tokens_map.items():\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {token_name}: '{token}' -> ID: {token_id}\")\n",
    "\n",
    "# Add custom special tokens if needed\n",
    "special_tokens_dict = {'additional_special_tokens': ['<CUSTOM>', '<MEDICAL>']}\n",
    "num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f'\\nAdded {num_added} special tokens.')\n",
    "\n",
    "# Visualize tokenization with special tokens\n",
    "text = \"Classify this sentence.\"\n",
    "encoded = tokenizer(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "print(f'\\nTokens with Special Tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization Algorithms <a id='algorithms'></a>\n",
    "\n",
    "Let's explore the three main tokenization algorithms: BPE, WordPiece, and Unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Byte Pair Encoding (BPE) - Used by GPT, RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokens: ['un', 'h', 'appiness']\n",
      "\n",
      "BPE tokenization examples:\n",
      "  'tokenization' -> ['token', 'ization']\n",
      "  'transformer' -> ['trans', 'former']\n",
      "  'preprocessing' -> ['pre', 'processing']\n",
      "  'pneumothorax' -> ['p', 'neum', 'oth', 'or', 'ax']\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa's BPE tokenizer\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = bpe_tokenizer.tokenize(text)\n",
    "print('BPE Tokens:', tokens)\n",
    "\n",
    "# Show how BPE handles various words\n",
    "test_words = ['tokenization', 'transformer', 'preprocessing', 'pneumothorax']\n",
    "print('\\nBPE tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = bpe_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 WordPiece - Used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece Tokens: ['un', '##ha', '##pp', '##iness']\n",
      "Notice the ## prefix for subword continuations!\n",
      "\n",
      "WordPiece tokenization examples:\n",
      "  'tokenization' -> ['token', '##ization']\n",
      "  'transformer' -> ['transform', '##er']\n",
      "  'preprocessing' -> ['prep', '##ro', '##ces', '##sing']\n",
      "  'pneumothorax' -> ['p', '##ne', '##um', '##otho', '##ra', '##x']\n"
     ]
    }
   ],
   "source": [
    "# Load BERT's WordPiece tokenizer\n",
    "wordpiece_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = wordpiece_tokenizer.tokenize(text)\n",
    "print('WordPiece Tokens:', tokens)\n",
    "print('Notice the ## prefix for subword continuations!')\n",
    "\n",
    "# Show how WordPiece handles various words\n",
    "print('\\nWordPiece tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = wordpiece_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Unigram - Used by XLNet, ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Tokens: ['‚ñÅunha', 'ppi', 'ness']\n",
      "\n",
      "Unigram tokenization examples:\n",
      "  'tokenization' -> ['‚ñÅto', 'ken', 'ization']\n",
      "  'transformer' -> ['‚ñÅtransform', 'er']\n",
      "  'preprocessing' -> ['‚ñÅpre', 'process', 'ing']\n",
      "  'pneumothorax' -> ['‚ñÅpneu', 'mot', 'hora', 'x']\n"
     ]
    }
   ],
   "source": [
    "# Load XLM-RoBERTa's Unigram tokenizer\n",
    "unigram_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "text = 'unhappiness'\n",
    "tokens = unigram_tokenizer.tokenize(text)\n",
    "print('Unigram Tokens:', tokens)\n",
    "\n",
    "# Show how Unigram handles various words\n",
    "print('\\nUnigram tokenization examples:')\n",
    "for word in test_words:\n",
    "    tokens = unigram_tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Algorithm Comparison\n",
    "\n",
    "Let's compare all three algorithms side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing: 'unhappiness'\n",
      "  BPE (RoBERTa): ['un', 'h', 'appiness'] (length: 3)\n",
      "  WordPiece (BERT): ['un', '##ha', '##pp', '##iness'] (length: 4)\n",
      "  Unigram (XLM-R): ['‚ñÅunha', 'ppi', 'ness'] (length: 3)\n",
      "\n",
      "Tokenizing: 'I love pizza! üçïüî•'\n",
      "  BPE (RoBERTa): ['I', 'ƒ†love', 'ƒ†pizza', '!', 'ƒ†√∞≈Å', 'ƒØ', 'ƒ∑', '√∞≈Å', 'ƒ∂', '¬•'] (length: 10)\n",
      "  WordPiece (BERT): ['i', 'love', 'pizza', '!', '[UNK]'] (length: 5)\n",
      "  Unigram (XLM-R): ['‚ñÅI', '‚ñÅlove', '‚ñÅpizza', '!', '‚ñÅ', 'üçï', 'üî•'] (length: 7)\n",
      "\n",
      "Tokenizing: 'COVID-19 pandemic'\n",
      "  BPE (RoBERTa): ['CO', 'VID', '-', '19', 'ƒ†pand', 'emic'] (length: 6)\n",
      "  WordPiece (BERT): ['co', '##vid', '-', '19', 'pan', '##de', '##mic'] (length: 7)\n",
      "  Unigram (XLM-R): ['‚ñÅCO', 'VID', '-19', '‚ñÅpande', 'mic'] (length: 5)\n",
      "\n",
      "Tokenizing: 'user@example.com'\n",
      "  BPE (RoBERTa): ['user', '@', 'example', '.', 'com'] (length: 5)\n",
      "  WordPiece (BERT): ['user', '@', 'example', '.', 'com'] (length: 5)\n",
      "  Unigram (XLM-R): ['‚ñÅuser', '@', 'ex', 'a', 'mple', '.', 'com'] (length: 7)\n",
      "\n",
      "Tokenizing: 'myocardial infarction'\n",
      "  BPE (RoBERTa): ['my', 'ocard', 'ial', 'ƒ†inf', 'ar', 'ction'] (length: 6)\n",
      "  WordPiece (BERT): ['my', '##oca', '##rdial', 'in', '##far', '##ction'] (length: 6)\n",
      "  Unigram (XLM-R): ['‚ñÅmy', 'o', 'card', 'ial', '‚ñÅin', 'far', 'ction'] (length: 7)\n"
     ]
    }
   ],
   "source": [
    "# Compare algorithms on various text types\n",
    "test_texts = [\n",
    "    \"unhappiness\",\n",
    "    \"I love pizza! üçïüî•\",\n",
    "    \"COVID-19 pandemic\",\n",
    "    \"user@example.com\",\n",
    "    \"myocardial infarction\"\n",
    "]\n",
    "\n",
    "tokenizers = {\n",
    "    'BPE (RoBERTa)': AutoTokenizer.from_pretrained('roberta-base'),\n",
    "    'WordPiece (BERT)': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'Unigram (XLM-R)': AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "}\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nTokenizing: '{text}'\")\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"  {name}: {tokens} (length: {len(tokens)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Tokenization <a id='custom'></a>\n",
    "\n",
    "For specialized domains, you might need to train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training a Custom Tokenizer (Simple Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Original BERT tokenization:\n",
      "['patient', 'exhibits', 'signs', 'of', 'p', '##ne', '##um', '##otho', '##ra', '##x', '.']\n",
      "\n",
      "Custom medical tokenization:\n",
      "['patient', 'exhibits', 'signs', 'of', 'pneumothorax', '.']\n"
     ]
    }
   ],
   "source": [
    "# Domain-specific medical texts\n",
    "medical_texts = [\n",
    "    \"Patient exhibits signs of pneumothorax.\",\n",
    "    \"CT scan reveals bilateral infiltrates.\",\n",
    "    \"Myocardial infarction confirmed via ECG.\",\n",
    "    \"Administered 5mg of morphine for pain management.\",\n",
    "    \"Post-operative recovery progressing normally.\",\n",
    "    \"CBC shows elevated white blood cell count.\",\n",
    "    \"MRI indicates herniated disc at L4-L5.\",\n",
    "    \"Patient history includes hypertension and diabetes.\",\n",
    "    \"Prescribed antibiotics for bacterial infection.\",\n",
    "    \"Radiology report shows no acute findings.\",\n",
    "    \"Chronic obstructive pulmonary disease exacerbation.\",\n",
    "    \"Electrocardiogram shows atrial fibrillation.\"\n",
    "]\n",
    "\n",
    "# Start with a base tokenizer as template\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Train a new tokenizer on domain data\n",
    "custom_tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    medical_texts,\n",
    "    vocab_size=1000,\n",
    ")\n",
    "\n",
    "# Test the custom tokenizer\n",
    "test_text = \"Patient exhibits signs of pneumothorax.\"\n",
    "print(\"Original BERT tokenization:\")\n",
    "print(base_tokenizer.tokenize(test_text))\n",
    "print(\"\\nCustom medical tokenization:\")\n",
    "print(custom_tokenizer.tokenize(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training a Custom BPE Tokenizer (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BPE tokens: ['[CLS]', 'Patient', 'w', 'it', 'h', 'm', 'y', 'ocar', 'dial', 'infarction', '[SEP]']\n",
      "BPE IDs: [2, 94, 39, 145, 27, 30, 41, 166, 205, 226, 3]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Initialize a tokenizer with BPE model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Pre-tokenization (splitting on whitespace and punctuation)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=1000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Train from our medical corpus\n",
    "tokenizer.train_from_iterator(medical_texts, trainer=trainer)\n",
    "\n",
    "# Add post-processing for BERT-style tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 2),\n",
    "        (\"[SEP]\", 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Test the custom BPE tokenizer\n",
    "test_text = \"Patient with myocardial infarction\"\n",
    "encoding = tokenizer.encode(test_text)\n",
    "print(f\"BPE tokens: {encoding.tokens}\")\n",
    "print(f\"BPE IDs: {encoding.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing General vs Domain-Specific Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing tokenization of medical terms:\n",
      "==================================================\n",
      "\n",
      "'pneumothorax':\n",
      "  General BERT: ['p', '##ne', '##um', '##otho', '##ra', '##x'] (length: 6)\n",
      "  Custom Medical: ['pneumothorax'] (length: 1)\n",
      "\n",
      "'myocardial':\n",
      "  General BERT: ['my', '##oca', '##rdial'] (length: 3)\n",
      "  Custom Medical: ['myocardial'] (length: 1)\n",
      "\n",
      "'electrocardiogram':\n",
      "  General BERT: ['electro', '##card', '##io', '##gram'] (length: 4)\n",
      "  Custom Medical: ['electrocardiogram'] (length: 1)\n",
      "\n",
      "'thrombocytopenia':\n",
      "  General BERT: ['th', '##rom', '##bo', '##cy', '##top', '##enia'] (length: 6)\n",
      "  Custom Medical: ['t', '##h', '##r', '##o', '##m', '##b', '##oc', '##y', '##t', '##o', '##p', '##e', '##nia'] (length: 13)\n",
      "\n",
      "'cholecystectomy':\n",
      "  General BERT: ['cho', '##le', '##cy', '##ste', '##ct', '##omy'] (length: 6)\n",
      "  Custom Medical: ['ch', '##o', '##l', '##e', '##c', '##y', '##st', '##e', '##c', '##t', '##o', '##m', '##y'] (length: 13)\n"
     ]
    }
   ],
   "source": [
    "# Medical terms that might be split differently\n",
    "medical_terms = [\n",
    "    \"pneumothorax\",\n",
    "    \"myocardial\",\n",
    "    \"electrocardiogram\",\n",
    "    \"thrombocytopenia\",\n",
    "    \"cholecystectomy\"\n",
    "]\n",
    "\n",
    "# Load general tokenizer\n",
    "general_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Comparing tokenization of medical terms:\")\n",
    "print(\"=\" * 50)\n",
    "for term in medical_terms:\n",
    "    general_tokens = general_tokenizer.tokenize(term)\n",
    "    custom_tokens = custom_tokenizer.tokenize(term)\n",
    "    \n",
    "    print(f\"\\n'{term}':\")\n",
    "    print(f\"  General BERT: {general_tokens} (length: {len(general_tokens)})\")\n",
    "    print(f\"  Custom Medical: {custom_tokens} (length: {len(custom_tokens)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Debugging and Visualization <a id='debugging'></a>\n",
    "\n",
    "Understanding how tokenization works is crucial for debugging NLP pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing Tokenization with Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Let's test: ü§ñ transformers!\"\n",
    "output = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=None\n",
    ")\n",
    "tokens = tokenizer.convert_ids_to_tokens(output['input_ids'])\n",
    "offsets = output['offset_mapping']\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(\"\\nToken breakdown:\")\n",
    "print(\"-\" * 50)\n",
    "for token, (start, end) in zip(tokens, offsets):\n",
    "    if start == end:  # Special tokens\n",
    "        print(f\"  {token:15} [SPECIAL TOKEN]\")\n",
    "    else:\n",
    "        print(f\"  {token:15} [{start:2}, {end:2}] -> '{text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detecting Tokenizer-Model Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using a mismatched tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_model = AutoModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"Tokenization mismatch!\"\n",
    "inputs = bert_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(\"Using BERT tokenizer with RoBERTa model:\")\n",
    "print(f\"BERT tokens: {bert_tokenizer.tokenize(text)}\")\n",
    "print(f\"BERT special tokens: {bert_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Show the correct pairing\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(f\"\\nRoBERTa tokens: {roberta_tokenizer.tokenize(text)}\")\n",
    "print(f\"RoBERTa special tokens: {roberta_tokenizer.special_tokens_map}\")\n",
    "print(\"\\n‚ö†Ô∏è  Notice the different special tokens and tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyzing Unknown Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Text with potentially unknown tokens\n",
    "test_texts = [\n",
    "    \"Normal English text\",\n",
    "    \"Emojis: üòÄ üöÄ ü§ñ\",\n",
    "    \"Special chars: ‚Ñ¢ ¬© ¬Æ ¬µ\",\n",
    "    \"Mixed: Hello‰∏ñÁïåBonjour\",\n",
    "    \"Medical: pneumonoultramicroscopicsilicovolcanoconiosis\",\n",
    "    \"Code: def foo(x): return x**2\",\n",
    "    \"Email: user@example.com\",\n",
    "    \"URL: https://example.com/path\"\n",
    "]\n",
    "\n",
    "print(\"Analyzing unknown token generation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Check for unknown tokens\n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    unk_count = token_ids.count(unk_token_id)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    if unk_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Contains {unk_count} unknown tokens!\")\n",
    "        unk_positions = [i for i, tid in enumerate(token_ids) if tid == unk_token_id]\n",
    "        print(f\"  Unknown at positions: {unk_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Debugging Padding and Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sentences of different lengths\n",
    "sentences = [\n",
    "    \"Short\",\n",
    "    \"This is a medium length sentence.\",\n",
    "    \"This is a much longer sentence that will definitely exceed the maximum length limit we set for truncation testing purposes.\"\n",
    "]\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "print(f\"Testing padding and truncation (max_length={max_length})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different padding strategies\n",
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])\n",
    "    print(f\"\\nOriginal: '{sent[:30]}...'\" if len(sent) > 30 else f\"\\nOriginal: '{sent}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Attention mask: {encoded['attention_mask'][i].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multimodal Tokenization <a id='multimodal'></a>\n",
    "\n",
    "Modern transformers can process not just text, but also images and other modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Image Tokenization with Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image for testing\n",
    "def create_sample_image():\n",
    "    image = Image.new('RGB', (224, 224), color='red')\n",
    "    # Add some variation\n",
    "    pixels = image.load()\n",
    "    for i in range(0, 224, 20):\n",
    "        for j in range(0, 224, 20):\n",
    "            pixels[i, j] = (0, 255, 0)  # Green dots\n",
    "    return image\n",
    "\n",
    "# Load a vision processor (tokenizer for images)\n",
    "processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Create sample image\n",
    "image = create_sample_image()\n",
    "print(f\"Image size: {image.size}\")\n",
    "\n",
    "# Process image into model-ready inputs\n",
    "inputs = processor(images=image, return_tensors='pt')\n",
    "print(f'\\nPixel values shape: {inputs[\"pixel_values\"].shape}')\n",
    "\n",
    "# Vision Transformer details\n",
    "patch_size = 16\n",
    "image_size = 224\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "print(f\"\\nViT tokenization details:\")\n",
    "print(f\"  Number of image patches: {num_patches}\")\n",
    "print(f\"  Each patch: {patch_size}x{patch_size} pixels\")\n",
    "print(f\"  Sequence length: {num_patches + 1} (patches + [CLS] token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 CLIP Multimodal Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP processor (handles both text and images)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Create sample image\n",
    "image = create_sample_image()\n",
    "\n",
    "# Sample texts for similarity comparison\n",
    "texts = [\n",
    "    \"a red square with green dots\",\n",
    "    \"a photo of a cat\", \n",
    "    \"a photo of a dog\",\n",
    "    \"a colorful pattern\"\n",
    "]\n",
    "\n",
    "# Process both text and images together\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"CLIP multimodal inputs:\")\n",
    "print(f\"  Text input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"  Image input shape: {inputs['pixel_values'].shape}\")\n",
    "\n",
    "# Show text tokenization\n",
    "print(\"\\nText tokenization:\")\n",
    "for i, text in enumerate(texts):\n",
    "    tokens = processor.tokenizer.convert_ids_to_tokens(inputs['input_ids'][i])\n",
    "    # Filter out padding tokens for display\n",
    "    tokens = [t for t in tokens if t != processor.tokenizer.pad_token]\n",
    "    print(f\"  '{text}': {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Comparing Different Image Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different image processors/tokenizers\n",
    "image = create_sample_image()\n",
    "\n",
    "processors = {\n",
    "    \"ViT\": \"google/vit-base-patch16-224\",\n",
    "    \"CLIP\": \"openai/clip-vit-base-patch32\",\n",
    "    \"DeiT\": \"facebook/deit-base-patch16-224\"\n",
    "}\n",
    "\n",
    "print(\"Comparing image processors:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model_name in processors.items():\n",
    "    try:\n",
    "        if name == \"CLIP\":\n",
    "            processor = CLIPProcessor.from_pretrained(model_name)\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        else:\n",
    "            processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        print(f\"\\n{name} processor ({model_name}):\")\n",
    "        print(f\"  Input shape: {inputs['pixel_values'].shape}\")\n",
    "        \n",
    "        # Get processor configuration\n",
    "        if hasattr(processor, 'size'):\n",
    "            print(f\"  Expected size: {processor.size}\")\n",
    "        if hasattr(processor, 'do_normalize'):\n",
    "            print(f\"  Normalization: {processor.do_normalize}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Exercises <a id='exercises'></a>\n",
    "\n",
    "Now let's implement the exercises from article5.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Performance Comparison of Tokenizers\n",
    "\n",
    "Let's compare the performance of different tokenizers to understand speed vs. functionality tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-Vocabulary (OOV) Handling\n",
    "print(\"=== Out-of-Vocabulary (OOV) Word Handling ===\")\n",
    "\n",
    "# Test with made-up and rare words\n",
    "test_texts = [\n",
    "    \"The flibbertigibbet jumped over the moon.\",\n",
    "    \"Pneumonoultramicroscopicsilicovolcanoconiosis is a lung disease.\",\n",
    "    \"The ü¶Ñ and üåà are beautiful.\",\n",
    "    \"Contact us at support@‰ºÅ‰∏ö.com\",\n",
    "]\n",
    "\n",
    "tokenizers_to_test = {\n",
    "    \"BERT\": bert_tokenizer,\n",
    "    \"GPT-2\": gpt2_tokenizer,\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "}\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, tokenizer in tokenizers_to_test.items():\n",
    "        # Get UNK token for this tokenizer\n",
    "        unk_token = getattr(tokenizer, 'unk_token', None)\n",
    "        \n",
    "        # Tokenize\n",
    "        if name == \"BERT\":\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "        else:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Check for UNK tokens\n",
    "        unk_count = tokens.count(unk_token) if unk_token else 0\n",
    "        \n",
    "        print(f\"{name:10} ({len(tokens):2} tokens): \", end=\"\")\n",
    "        if unk_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  {unk_count} UNK token(s)! \", end=\"\")\n",
    "        \n",
    "        # Show first few tokens\n",
    "        display_tokens = tokens[:8] + [\"...\"] if len(tokens) > 8 else tokens\n",
    "        print(display_tokens)\n",
    "\n",
    "# Demonstrate subword handling of OOV\n",
    "print(\"\\n=== Subword Decomposition of OOV Words ===\")\n",
    "made_up_word = \"supersupercalifragilisticexpialidocious\"\n",
    "\n",
    "print(f\"\\nMade-up word: '{made_up_word}'\")\n",
    "print(\"\\nHow different tokenizers handle it:\")\n",
    "print(f\"BERT:     {bert_tokenizer.tokenize(made_up_word)}\")\n",
    "print(f\"GPT-2:    {gpt2_tokenizer.tokenize(made_up_word)}\")\n",
    "print(f\"RoBERTa:  {tokenizers_to_test['RoBERTa'].tokenize(made_up_word)}\")\n",
    "\n",
    "# Key insight\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"- BERT uses [UNK] tokens for unknown words/characters\")\n",
    "print(\"- GPT-2 and RoBERTa use BPE to break down any word into known subwords\")\n",
    "print(\"- This is why BPE-based models handle OOV words better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Out-of-Vocabulary (OOV) Handling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tiktoken if not already installed\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    print(\"Installing tiktoken...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"tiktoken\"])\n",
    "    import tiktoken\n",
    "\n",
    "# Comparing with TikToken (used by GPT-3.5/4)\n",
    "print(\"=== Comparing HuggingFace Tokenizers with TikToken ===\")\n",
    "\n",
    "# Initialize tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 encoding\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"The transformer architecture revolutionized NLP in 2017.\",\n",
    "    \"def tokenize(text): return text.split()\",\n",
    "    \"Email: user@example.com, URL: https://example.com\",\n",
    "]\n",
    "\n",
    "# Compare tokenization\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # TikToken\n",
    "    tiktoken_ids = encoding.encode(text)\n",
    "    tiktoken_tokens = [encoding.decode([tid]) for tid in tiktoken_ids]\n",
    "    print(f\"TikToken (GPT-3.5/4): {tiktoken_tokens} ({len(tiktoken_tokens)} tokens)\")\n",
    "    \n",
    "    # GPT-2\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "    print(f\"GPT-2 (BPE): {gpt2_tokens} ({len(gpt2_tokens)} tokens)\")\n",
    "    \n",
    "    # BERT\n",
    "    bert_tokens = bert_tokenizer.tokenize(text.lower())\n",
    "    print(f\"BERT (WordPiece): {bert_tokens} ({len(bert_tokens)} tokens)\")\n",
    "\n",
    "# Vocabulary size comparison\n",
    "print(\"\\n=== Vocabulary Size Comparison ===\")\n",
    "print(f\"TikToken (cl100k_base): {encoding.n_vocab:,} tokens\")\n",
    "print(f\"GPT-2: {gpt2_tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"BERT: {bert_tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"T5: {t5_tokenizer.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Comparing Tokenizers with TikToken (GPT-3.5/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subword Tokenization Deep Dive\n",
    "print(\"=== Subword Tokenization Methods Comparison ===\")\n",
    "\n",
    "# Example text with various challenges\n",
    "text = (\n",
    "    \"Tokenization is fundamental to NLP. Let's explore BPE, WordPiece, and \"\n",
    "    \"SentencePiece algorithms!\"\n",
    ")\n",
    "\n",
    "# 1. BPE (Byte Pair Encoding) - GPT-2\n",
    "print(\"\\n1. BPE Tokenization (GPT-2):\")\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "gpt2_ids = gpt2_tokenizer.encode(text)\n",
    "print(f\"   Tokens: {gpt2_tokens}\")\n",
    "print(f\"   Token count: {len(gpt2_tokens)}\")\n",
    "print(f\"   Vocabulary size: {gpt2_tokenizer.vocab_size}\")\n",
    "\n",
    "# 2. WordPiece - BERT\n",
    "print(\"\\n2. WordPiece Tokenization (BERT):\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "bert_ids = bert_tokenizer.encode(text)\n",
    "print(f\"   Tokens: {bert_tokens}\")\n",
    "print(f\"   Token count: {len(bert_tokens)}\")\n",
    "print(f\"   Notice '##' prefix for subword continuations\")\n",
    "\n",
    "# 3. SentencePiece - T5\n",
    "print(\"\\n3. SentencePiece Tokenization (T5):\")\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "t5_tokens = t5_tokenizer.tokenize(text)\n",
    "t5_ids = t5_tokenizer.encode(text)\n",
    "print(f\"   Tokens: {t5_tokens}\")\n",
    "print(f\"   Token count: {len(t5_tokens)}\")\n",
    "print(f\"   Notice '‚ñÅ' for word boundaries\")\n",
    "\n",
    "# Handling unknown/rare words\n",
    "print(\"\\n=== Handling Unknown/Rare Words ===\")\n",
    "rare_word = \"Supercalifragilisticexpialidocious\"\n",
    "\n",
    "print(f\"\\nRare word: '{rare_word}'\")\n",
    "print(f\"GPT-2 (BPE): {gpt2_tokenizer.tokenize(rare_word)}\")\n",
    "print(f\"BERT (WordPiece): {bert_tokenizer.tokenize(rare_word.lower())}\")\n",
    "print(f\"T5 (SentencePiece): {t5_tokenizer.tokenize(rare_word)}\")\n",
    "\n",
    "# Demonstrate vocabulary lookup\n",
    "print(\"\\n=== Vocabulary Lookup Example ===\")\n",
    "word = \"tokenization\"\n",
    "print(f\"Looking up '{word}':\")\n",
    "\n",
    "# Check if whole word is in vocabulary\n",
    "if word in gpt2_tokenizer.vocab:\n",
    "    print(f\"  GPT-2: '{word}' is in vocabulary with ID {gpt2_tokenizer.vocab[word]}\")\n",
    "else:\n",
    "    print(f\"  GPT-2: '{word}' not in vocabulary, will be split into subwords\")\n",
    "\n",
    "# For BERT (lowercase)\n",
    "word_lower = word.lower()\n",
    "if word_lower in bert_tokenizer.vocab:\n",
    "    print(f\"  BERT: '{word_lower}' is in vocabulary with ID {bert_tokenizer.vocab[word_lower]}\")\n",
    "else:\n",
    "    print(f\"  BERT: '{word_lower}' not in vocabulary, will be split into subwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Subword Tokenization Deep Dive\n",
    "\n",
    "Let's explore how different subword tokenization methods handle complex words and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-to-Character Offset Mapping\n",
    "print(\"=== Token-to-Character Offset Mapping ===\")\n",
    "\n",
    "text = \"Hugging Face's tokenizers are extremely powerful!\"\n",
    "encoding = tokenizer(\n",
    "    text, \n",
    "    return_offsets_mapping=True, \n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "offsets = encoding['offset_mapping']\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(f\"\\nToken to character mapping:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Token':15} {'Text':20} {'Start':>6} {'End':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for token, (start, end) in zip(tokens, offsets):\n",
    "    if start == end:  # Special tokens have (0, 0) offsets\n",
    "        print(f\"{token:15} {'[SPECIAL TOKEN]':20} {start:6} {end:6}\")\n",
    "    else:\n",
    "        original_text = text[start:end]\n",
    "        print(f\"{token:15} {original_text:20} {start:6} {end:6}\")\n",
    "\n",
    "# Practical use case: Highlighting entities\n",
    "print(\"\\n=== Practical Example: Entity Highlighting ===\")\n",
    "\n",
    "# Simulate NER predictions (token indices for \"Hugging Face\")\n",
    "entity_token_indices = [1, 2]  # Tokens at positions 1 and 2\n",
    "\n",
    "print(\"Detected entity tokens:\")\n",
    "entity_chars = []\n",
    "for idx in entity_token_indices:\n",
    "    token = tokens[idx]\n",
    "    start, end = offsets[idx]\n",
    "    entity_chars.extend(range(start, end))\n",
    "    print(f\"  Token '{token}' -> '{text[start:end]}'\")\n",
    "\n",
    "# Reconstruct the entity from character positions\n",
    "min_char = min(entity_chars)\n",
    "max_char = max(entity_chars) + 1\n",
    "entity_text = text[min_char:max_char]\n",
    "print(f\"\\nExtracted entity: '{entity_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Token-to-Character Offset Mapping\n",
    "\n",
    "Offset mapping is crucial for tasks like Named Entity Recognition where you need to map model predictions back to the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Sequences for Question-Answering\n",
    "print(\"=== Multiple Sequences (Question-Answering) ===\")\n",
    "\n",
    "question = \"What is tokenization?\"\n",
    "context = (\n",
    "    \"Tokenization is the process of breaking down text into smaller units \"\n",
    "    \"called tokens. These tokens can be words, subwords, or even characters. \"\n",
    "    \"In NLP, tokenization is a crucial preprocessing step that converts \"\n",
    "    \"raw text into a format that machine learning models can understand.\"\n",
    ")\n",
    "\n",
    "# Encode question and context together\n",
    "qa_encoding = tokenizer(\n",
    "    question, context, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Convert to tokens to visualize\n",
    "tokens = tokenizer.convert_ids_to_tokens(qa_encoding[\"input_ids\"][0])\n",
    "token_type_ids = qa_encoding['token_type_ids'][0].tolist()\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Context: {context[:100]}...\")\n",
    "print(f\"\\nCombined tokens (first 20): {tokens[:20]}...\")\n",
    "print(f\"\\nToken type IDs visualization:\")\n",
    "print(\"  0 = Question/First sequence\")\n",
    "print(\"  1 = Context/Second sequence\")\n",
    "\n",
    "# Visualize token types\n",
    "for i in range(min(20, len(tokens))):\n",
    "    print(f\"  Token {i:2d}: '{tokens[i]:15}' -> Type {token_type_ids[i]}\")\n",
    "\n",
    "# Find where question ends and context begins\n",
    "sep_positions = [i for i, token in enumerate(tokens) if token == '[SEP]']\n",
    "print(f\"\\n[SEP] token positions: {sep_positions}\")\n",
    "print(f\"Question ends at position: {sep_positions[0]}\")\n",
    "print(f\"Context starts at position: {sep_positions[0] + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Handling Multiple Sequences (Question-Answering Example)\n",
    "\n",
    "Many NLP tasks require processing multiple sequences together, like question-answering or text entailment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Truncation Examples\n",
    "print(\"=== Advanced Truncation Examples ===\")\n",
    "\n",
    "# Create a very long text\n",
    "long_text = \" \".join([\"This is a very long sentence.\"] * 50)\n",
    "\n",
    "# Without truncation (will be very long)\n",
    "tokens_no_trunc = tokenizer.tokenize(long_text)\n",
    "print(f\"Without truncation: {len(tokens_no_trunc)} tokens\")\n",
    "\n",
    "# With truncation to max_length\n",
    "tokens_with_trunc = tokenizer(\n",
    "    long_text, truncation=True, max_length=20, return_tensors=\"pt\"\n",
    ")\n",
    "print(f\"With truncation (max_length=20): {tokens_with_trunc['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "# Show truncated tokens\n",
    "truncated_tokens = tokenizer.convert_ids_to_tokens(tokens_with_trunc['input_ids'][0].tolist())\n",
    "print(f\"Truncated tokens: {truncated_tokens}\")\n",
    "\n",
    "# Truncation strategies for sentence pairs\n",
    "print(\"\\n=== Truncation Strategies for Sentence Pairs ===\")\n",
    "question = \"What is the capital of France?\"\n",
    "context = \" \".join([\"Paris is the capital and most populous city of France.\"] * 10)\n",
    "\n",
    "# Strategy: 'only_second' - truncate only the context\n",
    "encoding_only_second = tokenizer(\n",
    "    question, context,\n",
    "    truncation='only_second',\n",
    "    max_length=50,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(f\"'only_second' strategy: {encoding_only_second['input_ids'].shape}\")\n",
    "\n",
    "# Strategy: 'longest_first' - truncate the longest sequence first\n",
    "encoding_longest_first = tokenizer(\n",
    "    question, context,\n",
    "    truncation='longest_first',\n",
    "    max_length=50,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(f\"'longest_first' strategy: {encoding_longest_first['input_ids'].shape}\")\n",
    "\n",
    "# Show which parts were kept\n",
    "tokens_only_second = tokenizer.convert_ids_to_tokens(encoding_only_second['input_ids'][0])\n",
    "print(f\"\\nTokens with 'only_second' (first 10): {tokens_only_second[:10]}...\")\n",
    "print(f\"Question preserved: {'what' in ' '.join(tokens_only_second).lower()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Advanced Truncation Strategies\n",
    "\n",
    "Truncation is essential when dealing with texts longer than the model's maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Padding Examples from Chapter 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 1. Padding examples with different strategies\n",
    "print(\"=== Advanced Padding Examples ===\")\n",
    "texts = [\n",
    "    \"Short text.\",\n",
    "    \"This is a medium length sentence that demonstrates padding.\",\n",
    "    \"This is a much longer sentence that will show how padding works with \"\n",
    "    \"multiple sentences of different lengths in a batch.\",\n",
    "]\n",
    "\n",
    "# Padding to max length in batch\n",
    "batch_encoding = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "print(f\"Original texts lengths: {[len(text.split()) for text in texts]}\")\n",
    "print(f\"Padded sequence lengths: {batch_encoding['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {batch_encoding['attention_mask'].shape}\")\n",
    "print(f\"\\nAttention masks (1=real token, 0=padding):\")\n",
    "for i, mask in enumerate(batch_encoding['attention_mask']):\n",
    "    print(f\"  Text {i+1}: {mask.tolist()}\")\n",
    "\n",
    "# Different padding strategies\n",
    "print(\"\\n=== Padding Strategies ===\")\n",
    "\n",
    "# Dynamic padding (to longest in batch)\n",
    "dynamic_padding = tokenizer(texts, padding=\"longest\", return_tensors=\"pt\")\n",
    "print(f\"Dynamic padding shape: {dynamic_padding['input_ids'].shape}\")\n",
    "\n",
    "# Fixed padding to specific length\n",
    "fixed_padding = tokenizer(texts, padding=\"max_length\", max_length=30, return_tensors=\"pt\")\n",
    "print(f\"Fixed padding shape (max_length=30): {fixed_padding['input_ids'].shape}\")\n",
    "\n",
    "# No padding\n",
    "no_padding = tokenizer(texts, padding=False)\n",
    "print(f\"No padding: {[len(ids) for ids in no_padding['input_ids']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Advanced Padding and Truncation Strategies\n",
    "\n",
    "Understanding how padding and truncation work is crucial for batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chapter 3 Advanced Tokenization Examples <a id='chapter3'></a>\n",
    "\n",
    "These are the advanced tokenization examples from Chapter 3 that were deferred to Chapter 5 for deeper explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises <a id='exercises'></a>\n",
    "\n",
    "Now let's implement the exercises from article5.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenize Multilingual Sentences with Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Tokenize multilingual sentences including emojis and domain-specific terms\n",
    "\n",
    "multilingual_sentences = [\n",
    "    \"Hello world! üëã\",\n",
    "    \"Bonjour le monde! üá´üá∑\",\n",
    "    \"Hola mundo! üá™üá∏\",\n",
    "    \"‰Ω†Â•Ω‰∏ñÁïåÔºÅüá®üá≥\",\n",
    "    \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! üá∑üá∫\",\n",
    "    \"The patient has COVID-19 ü¶†\",\n",
    "    \"Machine learning is amazing! ü§ñüí°\"\n",
    "]\n",
    "\n",
    "# Use multilingual BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "print(\"Exercise 1: Multilingual Tokenization Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sentence in multilingual_sentences:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\nText: '{sentence}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    \n",
    "    # Check for unknown tokens\n",
    "    if tokenizer.unk_token in tokens:\n",
    "        print(\"‚ö†Ô∏è  Contains unknown tokens!\")\n",
    "\n",
    "# Analysis of unusual tokenization results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis:\")\n",
    "print(\"- Emojis are often tokenized as [UNK] or split into multiple tokens\")\n",
    "print(\"- Different scripts (Chinese, Cyrillic) are handled differently\")\n",
    "print(\"- Domain terms like 'COVID-19' may be split unexpectedly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train a Custom BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training custom BPE tokenizer on scientific corpus...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Comparing standard GPT-2 vs custom scientific tokenizer:\n",
      "============================================================\n",
      "\n",
      "Term: 'quantum entanglement'\n",
      "  Standard GPT-2: ['quant', 'um', 'ƒ†ent', 'ang', 'lement'] (length: 5)\n",
      "  Custom Scientific: ['quantum', 'ƒ†entanglement'] (length: 2)\n",
      "\n",
      "Term: 'CRISPR-Cas9'\n",
      "  Standard GPT-2: ['CR', 'IS', 'PR', '-', 'Cas', '9'] (length: 6)\n",
      "  Custom Scientific: ['CRISPR', '-', 'Cas', '9'] (length: 4)\n",
      "\n",
      "Term: 'photosynthesis'\n",
      "  Standard GPT-2: ['photos', 'ynthesis'] (length: 2)\n",
      "  Custom Scientific: ['p', 'h', 'ot', 'osynthesis'] (length: 4)\n",
      "\n",
      "Term: 'neurotransmitters'\n",
      "  Standard GPT-2: ['ne', 'uro', 'trans', 'mit', 'ters'] (length: 5)\n",
      "  Custom Scientific: ['n', 'eu', 'rotrans', 'mitters'] (length: 4)\n",
      "\n",
      "Observation: Custom tokenizer better preserves domain-specific terms!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Train a custom BPE tokenizer on domain-specific corpus\n",
    "\n",
    "# Create a small domain-specific corpus (scientific abstracts)\n",
    "scientific_corpus = [\n",
    "    \"The quantum entanglement phenomenon demonstrates non-local correlations.\",\n",
    "    \"CRISPR-Cas9 enables precise genome editing in mammalian cells.\",\n",
    "    \"Machine learning algorithms optimize hyperparameters automatically.\",\n",
    "    \"Photosynthesis converts light energy into chemical energy.\",\n",
    "    \"Neurotransmitters facilitate synaptic transmission in neurons.\",\n",
    "    \"The Higgs boson was discovered at the Large Hadron Collider.\",\n",
    "    \"DNA polymerase synthesizes new DNA strands during replication.\",\n",
    "    \"Quantum computing leverages superposition and entanglement.\",\n",
    "    \"The mitochondria produces ATP through oxidative phosphorylation.\",\n",
    "    \"Climate change affects global temperature and precipitation patterns.\"\n",
    "]\n",
    "\n",
    "# Train custom tokenizer\n",
    "print(\"Training custom BPE tokenizer on scientific corpus...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "custom_tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    scientific_corpus,\n",
    "    vocab_size=1000\n",
    ")\n",
    "\n",
    "# Compare tokenization\n",
    "test_terms = [\n",
    "    \"quantum entanglement\",\n",
    "    \"CRISPR-Cas9\",\n",
    "    \"photosynthesis\",\n",
    "    \"neurotransmitters\"\n",
    "]\n",
    "\n",
    "print(\"\\nComparing standard GPT-2 vs custom scientific tokenizer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for term in test_terms:\n",
    "    standard_tokens = base_tokenizer.tokenize(term)\n",
    "    custom_tokens = custom_tokenizer.tokenize(term)\n",
    "    \n",
    "    print(f\"\\nTerm: '{term}'\")\n",
    "    print(f\"  Standard GPT-2: {standard_tokens} (length: {len(standard_tokens)})\")\n",
    "    print(f\"  Custom Scientific: {custom_tokens} (length: {len(custom_tokens)})\")\n",
    "\n",
    "print(\"\\nObservation: Custom tokenizer better preserves domain-specific terms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Identify and Fix Tokenization Mismatch Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3: Tokenizer-Model Mismatch Detection\n",
      "============================================================\n",
      "\n",
      "Step 1: Creating intentional mismatch...\n",
      "BERT tokenizer output:\n",
      "  Tokens: ['token', '##ization', 'is', 'crucial', 'for', 'nl', '##p', '!']\n",
      "  Special tokens: ['unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token']\n",
      "  Input IDs shape: torch.Size([1, 10])\n",
      "\n",
      "Step 2: Fixing the mismatch...\n",
      "RoBERTa tokenizer output (correct):\n",
      "  Tokens: ['Token', 'ization', 'ƒ†is', 'ƒ†crucial', 'ƒ†for', 'ƒ†N', 'LP', '!']\n",
      "  Special tokens: ['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token']\n",
      "  Input IDs shape: torch.Size([1, 10])\n",
      "\n",
      "Step 3: Key differences:\n",
      "- BERT uses [CLS] and [SEP] tokens\n",
      "- RoBERTa uses <s> and </s> tokens\n",
      "- Different vocabulary mappings\n",
      "- Different tokenization rules\n",
      "\n",
      "‚úÖ Always match tokenizer with model architecture!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Intentionally create and fix a tokenizer-model mismatch\n",
    "\n",
    "print(\"Exercise 3: Tokenizer-Model Mismatch Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create the mismatch\n",
    "print(\"\\nStep 1: Creating intentional mismatch...\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Note: We're not loading the full model to save memory in the notebook\n",
    "# In practice, you would use: roberta_model = AutoModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"Tokenization is crucial for NLP!\"\n",
    "\n",
    "# Show the problem\n",
    "bert_tokens = bert_tokenizer(text, return_tensors='pt')\n",
    "print(f\"BERT tokenizer output:\")\n",
    "print(f\"  Tokens: {bert_tokenizer.tokenize(text)}\")\n",
    "print(f\"  Special tokens: {list(bert_tokenizer.special_tokens_map.keys())}\")\n",
    "print(f\"  Input IDs shape: {bert_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Step 2: Fix the mismatch\n",
    "print(\"\\nStep 2: Fixing the mismatch...\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "roberta_tokens = roberta_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(f\"RoBERTa tokenizer output (correct):\")\n",
    "print(f\"  Tokens: {roberta_tokenizer.tokenize(text)}\")\n",
    "print(f\"  Special tokens: {list(roberta_tokenizer.special_tokens_map.keys())}\")\n",
    "print(f\"  Input IDs shape: {roberta_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Step 3: Key differences\n",
    "print(\"\\nStep 3: Key differences:\")\n",
    "print(\"- BERT uses [CLS] and [SEP] tokens\")\n",
    "print(\"- RoBERTa uses <s> and </s> tokens\")\n",
    "print(\"- Different vocabulary mappings\")\n",
    "print(\"- Different tokenization rules\")\n",
    "print(\"\\n‚úÖ Always match tokenizer with model architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Visualize Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Visualize special tokens for classification task\n",
    "\n",
    "print(\"Exercise 4: Special Tokens Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Text classification example\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Single sentence\n",
    "single_text = \"This movie is fantastic!\"\n",
    "single_encoded = tokenizer(single_text)\n",
    "\n",
    "print(\"Single sentence classification:\")\n",
    "print(f\"Text: '{single_text}'\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(single_encoded['input_ids'])\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"  - [CLS] at position 0: Classification token\")\n",
    "print(f\"  - Content tokens: {tokens[1:-1]}\")\n",
    "print(f\"  - [SEP] at position {len(tokens)-1}: Separator token\")\n",
    "\n",
    "# Sentence pair (for tasks like entailment)\n",
    "text_a = \"The weather is sunny.\"\n",
    "text_b = \"It's a beautiful day.\"\n",
    "pair_encoded = tokenizer(text_a, text_b)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"\\nSentence pair classification:\")\n",
    "print(f\"Text A: '{text_a}'\")\n",
    "print(f\"Text B: '{text_b}'\")\n",
    "pair_tokens = tokenizer.convert_ids_to_tokens(pair_encoded['input_ids'])\n",
    "token_type_ids = pair_encoded['token_type_ids']\n",
    "\n",
    "print(f\"\\nTokens with segment IDs:\")\n",
    "for i, (token, segment) in enumerate(zip(pair_tokens, token_type_ids)):\n",
    "    print(f\"  {i:2d}: '{token:15}' (segment {segment})\")\n",
    "\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"  - [CLS]: Start of sequence, used for classification\")\n",
    "print(f\"  - First [SEP]: Separates sentence A from sentence B\")\n",
    "print(f\"  - Second [SEP]: End of sequence\")\n",
    "print(f\"  - Segment 0: First sentence + special tokens\")\n",
    "print(f\"  - Segment 1: Second sentence + final [SEP]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Experiment with Noisy Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Tokenizing noisy text with different algorithms\n",
    "\n",
    "print(\"Exercise 5: Noisy Text Tokenization Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Noisy text examples\n",
    "noisy_texts = [\n",
    "    # Typos\n",
    "    \"I lvoe naturl langauge procesing\",\n",
    "    \"Thsi is vrey interessting\",\n",
    "    \n",
    "    # Slang and informal\n",
    "    \"gonna b late 2nite lol\",\n",
    "    \"ur awesome btw :)\",\n",
    "    \n",
    "    # Code snippets\n",
    "    \"def calculate_loss(y_true, y_pred): return mse(y_true, y_pred)\",\n",
    "    \"import torch.nn as nn\",\n",
    "    \n",
    "    # Mixed case and special chars\n",
    "    \"CamelCaseExample_with_underscores\",\n",
    "    \"email@domain.com | phone: +1-234-567-8900\"\n",
    "]\n",
    "\n",
    "# Load different tokenizers\n",
    "tokenizers = {\n",
    "    'BPE (GPT-2)': AutoTokenizer.from_pretrained('gpt2'),\n",
    "    'WordPiece (BERT)': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'Unigram (XLM-R)': AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "}\n",
    "\n",
    "for text in noisy_texts:\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text.lower() if 'uncased' in name else text)\n",
    "        results[name] = {\n",
    "            'tokens': tokens,\n",
    "            'length': len(tokens)\n",
    "        }\n",
    "        print(f\"{name}: {tokens} (length: {len(tokens)})\")\n",
    "    \n",
    "    # Find which handles it best (fewer tokens usually = better)\n",
    "    best = min(results.items(), key=lambda x: x[1]['length'])\n",
    "    print(f\"\\n‚úì Most efficient: {best[0]} with {best[1]['length']} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis:\")\n",
    "print(\"- BPE (GPT-2) handles contractions and slang well\")\n",
    "print(\"- WordPiece (BERT) struggles with typos and creates more [UNK] tokens\")\n",
    "print(\"- Unigram (XLM-R) provides a balance for multilingual text\")\n",
    "print(\"- Code snippets are challenging for all tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. **Basic Tokenization**: Converting text to tokens and IDs\n",
    "2. **Tokenization Algorithms**: BPE, WordPiece, and Unigram differences\n",
    "3. **Custom Tokenization**: Training tokenizers for specialized domains\n",
    "4. **Debugging Tools**: Visualizing and understanding tokenization\n",
    "5. **Multimodal Tokenization**: Processing images alongside text\n",
    "\n",
    "### Key Points to Remember:\n",
    "\n",
    "- Always match your tokenizer with your model\n",
    "- Custom tokenizers can significantly improve domain-specific performance\n",
    "- Different algorithms have different strengths (BPE for flexibility, WordPiece for consistency)\n",
    "- Special tokens are crucial for task-specific fine-tuning\n",
    "- Modern transformers can handle multiple modalities through specialized tokenization\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try training a custom tokenizer on your own domain data\n",
    "- Experiment with different tokenization strategies for your use case\n",
    "- Explore multimodal models like CLIP for vision-language tasks\n",
    "- Practice debugging tokenization issues in your NLP pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
