"""
Chapter 5: Multimodal Tokenization
Image and multimodal tokenization examples
"""

import requests
from PIL import Image
from transformers import AutoImageProcessor, AutoProcessor, CLIPProcessor

from config import DATA_DIR, logger


def download_sample_image():
    """Download a sample image for testing"""
    image_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/image_classification.png"

    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()
        image = Image.open(response.raw)

        # Save image locally for offline use
        image_path = DATA_DIR / "sample_image.png"
        image_path.parent.mkdir(parents=True, exist_ok=True)
        image.save(image_path)

        logger.info(f"Downloaded sample image to: {image_path}")
        return image
    except Exception as e:
        logger.warning(f"Failed to download image: {e}")
        # Create a simple test image as fallback
        image = Image.new("RGB", (224, 224), color="red")
        return image


def image_tokenization_example():
    """Image tokenization with Vision Transformer (from article5.md lines 289-305)"""
    logger.info("=== Image Tokenization with ViT ===")

    # Load a vision processor (tokenizer for images)
    processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

    # Get sample image
    image = download_sample_image()
    logger.info(f"Image size: {image.size}")

    # Process image into model-ready inputs
    inputs = processor(images=image, return_tensors="pt")
    logger.info(f'Pixel values shape: {inputs["pixel_values"].shape}')

    # Vision Transformer details
    patch_size = 16
    image_size = 224
    num_patches = (image_size // patch_size) ** 2
    logger.info(f"Number of image patches: {num_patches}")
    logger.info(f"Each patch: {patch_size}x{patch_size} pixels")

    return inputs


def clip_multimodal_tokenization():
    """CLIP multimodal tokenization example (mentioned in article5.md line 315-320)"""
    logger.info("\n=== CLIP Multimodal Tokenization ===")

    # Load CLIP processor (handles both text and images)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    # Get sample image
    image = download_sample_image()

    # Sample texts for similarity comparison
    texts = [
        "a photo of a cat",
        "a photo of a dog",
        "a diagram showing AI concepts",
        "a beautiful landscape",
    ]

    # Process both text and images together
    inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

    logger.info("CLIP inputs:")
    logger.info(f"  Text input shape: {inputs['input_ids'].shape}")
    logger.info(f"  Image input shape: {inputs['pixel_values'].shape}")

    # Show text tokenization
    for i, text in enumerate(texts):
        tokens = processor.tokenizer.convert_ids_to_tokens(inputs["input_ids"][i])
        logger.info(f"  Text '{text}' tokens: {tokens}")

    return inputs


def vision_encoder_decoder_example():
    """Vision Encoder-Decoder model tokenization"""
    logger.info("\n=== Vision Encoder-Decoder Tokenization ===")

    try:
        # Load processor for vision-to-text model
        processor = AutoProcessor.from_pretrained("microsoft/trocr-base-handwritten")

        # Create a simple test image with text
        image = Image.new("RGB", (384, 64), color="white")

        # Process image
        inputs = processor(images=image, return_tensors="pt")

        logger.info("TrOCR (Transformer OCR) inputs:")
        logger.info(f"  Pixel values shape: {inputs['pixel_values'].shape}")

        # The model would generate text tokens as output
        logger.info("  Output: Text tokens (generated by decoder)")

    except Exception as e:
        logger.warning(f"Could not load TrOCR model: {e}")
        logger.info("Skipping vision encoder-decoder example")


def compare_image_processors():
    """Compare different image processors/tokenizers"""
    logger.info("\n=== Comparing Image Processors ===")

    # Get sample image
    image = download_sample_image()

    processors = {
        "ViT": "google/vit-base-patch16-224",
        "CLIP": "openai/clip-vit-base-patch32",
        "DeiT": "facebook/deit-base-patch16-224",
    }

    for name, model_name in processors.items():
        try:
            if name == "CLIP":
                processor = CLIPProcessor.from_pretrained(model_name)
                inputs = processor(images=image, return_tensors="pt")
            else:
                processor = AutoImageProcessor.from_pretrained(model_name)
                inputs = processor(images=image, return_tensors="pt")

            logger.info(f"\n{name} processor:")
            logger.info(f"  Input shape: {inputs['pixel_values'].shape}")

            # Get processor configuration
            if hasattr(processor, "size"):
                logger.info(f"  Expected size: {processor.size}")
            if hasattr(processor, "do_normalize"):
                logger.info(f"  Normalization: {processor.do_normalize}")

        except Exception as e:
            logger.warning(f"Could not load {name} processor: {e}")


def multimodal_batch_processing():
    """Batch processing for multimodal inputs"""
    logger.info("\n=== Multimodal Batch Processing ===")

    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    # Multiple images (create simple test images)
    images = [
        Image.new("RGB", (224, 224), color="red"),
        Image.new("RGB", (224, 224), color="blue"),
        Image.new("RGB", (224, 224), color="green"),
    ]

    # Multiple texts
    texts = ["a red square", "a blue square", "a green square"]

    # Batch process
    inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)

    logger.info("Batch processing results:")
    logger.info(f"  Number of images: {len(images)}")
    logger.info(f"  Number of texts: {len(texts)}")
    logger.info(f"  Text batch shape: {inputs['input_ids'].shape}")
    logger.info(f"  Image batch shape: {inputs['pixel_values'].shape}")

    return inputs


def image_preprocessing_details():
    """Show detailed image preprocessing steps"""
    logger.info("\n=== Image Preprocessing Details ===")

    processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

    # Create test image
    image = Image.new("RGB", (300, 400), color="purple")
    logger.info(f"Original image size: {image.size}")

    # Manual preprocessing steps (what the processor does internally)
    logger.info("\nPreprocessing steps:")
    logger.info("1. Resize to expected dimensions")
    logger.info("2. Convert to tensor")
    logger.info("3. Normalize pixel values")
    logger.info("4. Arrange into patches")

    # Process with details
    inputs = processor(images=image, return_tensors="pt")

    # Show configuration
    logger.info("\nProcessor configuration:")
    logger.info(f"  Size: {processor.size}")
    logger.info(f"  Resample method: {processor.resample}")
    logger.info(f"  Do rescale: {processor.do_rescale}")
    logger.info(f"  Rescale factor: {getattr(processor, 'rescale_factor', 'N/A')}")
    logger.info(f"  Do normalize: {processor.do_normalize}")
    if hasattr(processor, "image_mean") and hasattr(processor, "image_std"):
        logger.info(f"  Normalization mean: {processor.image_mean}")
        logger.info(f"  Normalization std: {processor.image_std}")

    logger.info(f"\nProcessed tensor shape: {inputs['pixel_values'].shape}")

    return inputs


def run_all_examples():
    """Run all multimodal tokenization examples"""
    logger.info("Running Multimodal Tokenization Examples from Chapter 5")

    # Basic image tokenization
    image_tokenization_example()

    # CLIP multimodal
    clip_multimodal_tokenization()

    # Vision encoder-decoder
    vision_encoder_decoder_example()

    # Compare processors
    compare_image_processors()

    # Batch processing
    multimodal_batch_processing()

    # Preprocessing details
    image_preprocessing_details()

    logger.info("\nâœ… All multimodal tokenization examples completed!")


if __name__ == "__main__":
    run_all_examples()
